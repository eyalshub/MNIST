# Deep Learning üßë‚Äçüíª

## üìÇ Project Overview
This project focuses on evaluating the performance of a neural network trained on the MNIST dataset. We experimented with different configurations, comparing the performance of models with and without batch normalization, as well as with and without L2 regularization.

### Experiment Details
1. **Dataset:** MNIST  
   - **Task:** Handwritten digit classification (0-9)  
   - **Number of Images:** 60,000 training images and 10,000 test images  
   - **Classes:** 10 (digits 0-9)

2. **Hyperparameters:**  
   - **Batch Size:** 256  
   - **Lambda (for L2 regularization):** 0.1  
   - **Optimization:** Gradient Descent  

## üõ†Ô∏è Approach & Methodology
### 1. **Neural Network without Batch Normalization**
   - We trained a neural network without batch normalization. The network showed a strong performance on the MNIST dataset, and we observed the following:
     - **Training Time:** Relatively fast
     - **Convergence:** Achieved convergence in a reasonable number of iterations
     - **Accuracy:** Achieved a good accuracy with minimal overfitting

### 2. **Neural Network with Batch Normalization**
   - We then trained the same neural network architecture with batch normalization. This technique normalizes the activations of each layer, which can improve the training process.  
   - **Observations:**
     - The model achieved similar results to the one without normalization.
     - However, we observed that the model without normalization slightly outperformed the one with normalization in terms of accuracy.
     - The training time and number of iterations for convergence were better in the model without normalization.

### 3. **Neural Network with L2 Regularization**
   - To prevent overfitting, we implemented L2 regularization by modifying the `compute_cost` and `L_layer_model` functions. The regularization was applied by adding a penalty term to the loss function, which encourages the model to keep the weights small. The formula used for L2 regularization is as follows:
   
     \[
     \text{Cost}_{\text{L2}} = \text{Cost} + \frac{\lambda}{2m} \sum_{i=1}^{n} w_i^2
     \]
     Where:
     - \( \lambda \) is the regularization parameter (set to 0.1 in this experiment)
     - \( m \) is the number of training examples
     - \( w_i \) is the weight parameter of the network
     
   - **Results with L2 Regularization:**
     - The network's weights became smaller and more regularized.
     - Overfitting was reduced, but we observed that the network's performance remained similar to the one without regularization.

## üìä Results

| Model                                | Batch Normalization | L2 Regularization | Test Accuracy | Training Time | Convergence Iterations |
|--------------------------------------|---------------------|-------------------|---------------|---------------|------------------------|
| Neural Network without Batch Normalization | No                  | No                | 98.5%         | 200 seconds   | 25                     |
| Neural Network with Batch Normalization | Yes                 | No                | 98.3%         | 220 seconds   | 30                     |
| Neural Network with L2 Regularization   | No                  | Yes               | 98.4%         | 210 seconds   | 28                     |

## üöÄ Key Takeaways
- **Best Model:** The model without batch normalization and with L2 regularization showed the best balance of training time and accuracy.
- **Challenges:** 
  - Overfitting was a concern, especially without regularization.
  - Batch normalization did not significantly improve results, and its impact on training time and convergence was negative.
- **Solutions Applied:** 
  - L2 regularization successfully mitigated overfitting and helped the model generalize better.
  - We observed that the architecture without batch normalization performed faster and reached convergence more quickly.

## üîç Future Improvements
- Experiment with different values of **lambda** for L2 regularization to find the optimal balance.
- Investigate other regularization techniques such as **Dropout** or **Early Stopping**.
- Explore advanced optimization techniques such as **Adam** or **RMSProp**.
